# -*- coding: utf-8 -*-
"""Proyek Akhir NEW lagi_Meakhel Gunawan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IlUENprn09de8qelv9pDa6D8Qagaj6X5

# **Proyek Klasifikasi Gambar**
## **Data Diri**
- **Nama:** Meakhel Gunawan
- **Kampus:** Universitas Negeri Surabaya Teknik Informatika (Semester 6)
- **Email:** meakhel220504@gmail.com
- **ID Dicoding:** meakhelg
- **Dataset:** https://www.kaggle.com/datasets/borhanitrash/animal-image-classification-dataset

# <font color='yellow'> **Import Library**</font>
"""

!pip install -q kaggle

!pip install split-folders

!pip install tensorflowjs

from google.colab import userdata
import zipfile, os
from PIL import Image
import splitfolders
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten
from keras.models import Sequential
from tensorflow import keras
from tensorflow.keras import layers
import tensorflowjs as tfjs
from google.colab import files
from keras.preprocessing import image
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np

"""#  <font color='yellow'> **Download Dataset dari Kaggle**</font>



"""

os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')

!kaggle datasets download -d borhanitrash/animal-image-classification-dataset
! unzip animal-image-classification-dataset.zip

"""#  <font color='yellow'> **Data Preparation**</font>"""

def print_images_resolution(directory):
    unique_sizes = set()
    total_images = 0

    for subdir in os.listdir(directory):
        subdir_path = os.path.join(directory, subdir)
        image_files = os.listdir(subdir_path)
        num_images = len(image_files)
        print(f"{subdir}: {num_images}")
        total_images += num_images

        for img_file in image_files:
            img_path = os.path.join(subdir_path, img_file)
            with Image.open(img_path) as img:
                unique_sizes.add(img.size)

        for size in unique_sizes:
            print(f"- {size}")
        print("---------------")

    print(f"\nTotal: {total_images}")

print_images_resolution('/content/Animals')

# Path folder dataset asal
input_folder = "/content/Animals"

# Output folder untuk dataset yang sudah dibagi
output_folder = "/content/Animals Classification"

# Membagi dataset dengan rasio 80% train, 10% test, 10% val
splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(0.8, 0.1, 0.1), group_prefix=None, move=False)

print("\n Dataset berhasil dibagi ke dalam train, test, dan val!")

os.listdir("/content/Animals Classification")

os.listdir("/content/Animals Classification/train")

os.listdir("/content/Animals Classification/test")

os.listdir("/content/Animals Classification/val")

# Path ke dataset yang sudah dibagi
train_dir = "/content/Animals Classification/train"
test_dir = "/content/Animals Classification/test"
val_dir = "/content/Animals Classification/val"

# Data Augmentation untuk training
train_datagen = ImageDataGenerator(
    rescale=1./255,        # Normalisasi pixel (0-255 menjadi 0-1)
    rotation_range=20,     # Rotasi gambar hingga 20 derajat
    width_shift_range=0.2, # Pergeseran horizontal
    height_shift_range=0.2,# Pergeseran vertikal
    shear_range=0.2,       # Distorsi bentuk gambar
    zoom_range=0.2,        # Zoom in/out gambar
    horizontal_flip=True,  # Membalik gambar secara horizontal
    fill_mode='nearest'    # Mengisi piksel kosong dengan metode 'nearest'
)

# Generator untuk validation dan test (tanpa augmentasi)
val_test_datagen = ImageDataGenerator(rescale=1./255)

# Train Generator
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),  # Sesuai dengan ukuran input model
    batch_size=32,
    class_mode='categorical'  # Karena lebih dari 2 kelas
)

# Validation Generator
val_generator = val_test_datagen.flow_from_directory(
    val_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'  # Karena lebih dari 2 kelas
)

# Test Generator
test_generator = val_test_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',  # Karena lebih dari 2 kelas
    shuffle=False  # Tidak perlu diacak untuk evaluasi
)

"""#  <font color='yellow'> **Modelling**</font>"""

# Load MobileNetV2 tanpa top layers, sebagai feature extractor
base_model = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))
base_model.trainable = False

# Membangun model dengan tambahan Conv2D & Pooling setelah feature extractor
model = keras.Sequential([
    base_model,  # MobileNetV2 sebagai backbone

    # Tambahkan extra Conv2D & Pooling untuk lebih banyak fitur
    layers.Conv2D(32, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(2,2),

    # Global Average Pooling untuk mengurangi dimensi
    layers.GlobalAveragePooling2D(),

    # Fully Connected Layers (Dense)
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.7),
    layers.Dense(3, activation='softmax')  # Output 3 kelas
])

# Compile Model
model.compile(loss='categorical_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=0.0001),
              metrics=['accuracy'])

# Callback untuk menghentikan training jika akurasi > 95% pada training dan validation
class EarlyStopping_95(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('accuracy') > 0.95 and logs.get('val_accuracy') > 0.95:
            print(f"\nAkurasi telah mencapai 95% pada epoch {epoch+1}, menghentikan training...")
            self.model.stop_training = True

# Inisialisasi callback
callbacks = EarlyStopping_95()

history = model.fit(
      train_generator,
      epochs=30,
      validation_data=val_generator,
      validation_steps=5,
      verbose=2,
      callbacks=[callbacks]
      )

# Ringkasan Model
model.summary()

"""#  <font color='yellow'> **Evaluasi dan Visualisasi**</font>"""

# Plot akurasi
plt.figure(figsize=(8,5))
plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue', linestyle='solid', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linestyle='dashed', marker='s')

# Tambahkan title dan label
plt.title("Perkembangan Akurasi Model", fontsize=14)
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)

# Tambahkan grid untuk memperjelas pola
plt.grid(True, linestyle="--", alpha=0.6)

# Tambahkan legenda
plt.legend(loc='lower right')

# Cari akurasi terbaik
best_train_acc = max(history.history['accuracy'])
best_val_acc = max(history.history['val_accuracy'])

# Tambahkan anotasi akurasi terbaik pada grafik
plt.annotate(f"Train Max: {best_train_acc:.2%}",
             xy=(history.history['accuracy'].index(best_train_acc), best_train_acc),
             xytext=(-30, 10), textcoords="offset points",
             arrowprops=dict(arrowstyle="->", color="blue"), color="blue")

plt.annotate(f"Val Max: {best_val_acc:.2%}",
             xy=(history.history['val_accuracy'].index(best_val_acc), best_val_acc),
             xytext=(-30, -20), textcoords="offset points",
             arrowprops=dict(arrowstyle="->", color="red"), color="red")

# Tampilkan plot
plt.show()

# Tampilkan akurasi terbaik
print(f"Akurasi terbaik model: {best_train_acc:.2%} pada training dan {best_val_acc:.2%} pada validation.")

# Plot loss
plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Train Loss', color='blue', linestyle='solid', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linestyle='dashed', marker='s')

# Tambahkan title dan label
plt.title("Perkembangan Loss Model", fontsize=14)
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Loss", fontsize=12)

# Tambahkan grid untuk memperjelas pola
plt.grid(True, linestyle="--", alpha=0.6)

# Tambahkan legenda
plt.legend(loc='upper right')

# Cari loss terbaik (terendah)
best_train_loss = min(history.history['loss'])
best_val_loss = min(history.history['val_loss'])

# Tambahkan anotasi loss terendah pada grafik
plt.annotate(f"Train Min: {best_train_loss:.4f}",
             xy=(history.history['loss'].index(best_train_loss), best_train_loss),
             xytext=(-30, 10), textcoords="offset points",
             arrowprops=dict(arrowstyle="->", color="blue"), color="blue")

plt.annotate(f"Val Min: {best_val_loss:.4f}",
             xy=(history.history['val_loss'].index(best_val_loss), best_val_loss),
             xytext=(-30, -20), textcoords="offset points",
             arrowprops=dict(arrowstyle="->", color="red"), color="red")

# Tampilkan plot
plt.show()

# Tampilkan loss terbaik
print(f"Loss terbaik model: {best_train_loss:.4f} pada training dan {best_val_loss:.4f} pada validation.")

"""#  <font color='yellow'> **Konversi Model (Menyimpan Model)**</font>"""

# Menyimpan Model dalam Format SavedModel
model.save("model.keras")  # Format default Keras 3
model.save("model.h5")
model.export("model_saved")

# Mengonversi ke Format TensorFlow Lite (TFLite)
converter = tf.lite.TFLiteConverter.from_saved_model("model_saved")
tflite_model = converter.convert()

# Simpan model TF-Lite ke file .tflite
with open("model.tflite", "wb") as f:
    f.write(tflite_model)

# Mengonversi ke Format TensorFlow.js (TFJS)
tfjs.converters.save_keras_model(model, "model_tfjs")

"""#  <font color='yellow'> **Inference atau Testing**</font>"""

# Load model dari file
model = tf.keras.models.load_model("model.keras")  # Sesuaikan dengan lokasi file model

class_labels = ["Kucing", "Anjing", "Ular"]

"""Bentuk Folder (Animals Classification/test/)"""

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    "/content/Animals Classification/test",  # Sesuaikan dengan lokasi dataset uji
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

test_loss, test_acc = model.evaluate(test_generator)
print(f"Akurasi model pada dataset uji: {test_acc * 100:.2f}%")

"""Bentuk Upload Gambar"""

# Unggah file gambar dari komputer
uploaded = files.upload()

# Ambil nama file yang diunggah
image_path = list(uploaded.keys())[0]
print(f"Gambar yang diunggah: {image_path}")

# Fungsi prediksi gambar
def predict_uploaded_image(image_path, model, target_size=(150, 150)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Normalisasi

    # Prediksi
    prediction = model.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)[0]
    predicted_label = class_labels[predicted_class]

    return predicted_label, prediction

# Lakukan prediksi
predicted_label, prediction = predict_uploaded_image(image_path, model)
print(f"Prediksi kelas: {predicted_label}, Probabilitas: {prediction}")

# Unggah file gambar dari komputer
uploaded = files.upload()

# Ambil nama file yang diunggah
image_path = list(uploaded.keys())[0]
print(f"Gambar yang diunggah: {image_path}")

# Fungsi prediksi gambar
def predict_uploaded_image(image_path, model, target_size=(150, 150)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Normalisasi

    # Prediksi
    prediction = model.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)[0]
    predicted_label = class_labels[predicted_class]

    return predicted_label, prediction

# Lakukan prediksi
predicted_label, prediction = predict_uploaded_image(image_path, model)
print(f"Prediksi kelas: {predicted_label}, Probabilitas: {prediction}")

# Unggah file gambar dari komputer
uploaded = files.upload()

# Ambil nama file yang diunggah
image_path = list(uploaded.keys())[0]
print(f"Gambar yang diunggah: {image_path}")

# Fungsi prediksi gambar
def predict_uploaded_image(image_path, model, target_size=(150, 150)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Normalisasi

    # Prediksi
    prediction = model.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)[0]
    predicted_label = class_labels[predicted_class]

    return predicted_label, prediction

# Lakukan prediksi
predicted_label, prediction = predict_uploaded_image(image_path, model)
print(f"Prediksi kelas: {predicted_label}, Probabilitas: {prediction}")